using FluxIndex.Extensions.WebFlux.Models;
using Microsoft.Extensions.Logging;
using System.Text.RegularExpressions;
using HtmlAgilityPack;

namespace FluxIndex.Extensions.WebFlux.Services;

/// <summary>
/// Basic web content processor implementation
/// </summary>
public class BasicWebContentProcessor : IWebContentProcessor
{
    private readonly HttpClient _httpClient;
    private readonly ILogger<BasicWebContentProcessor> _logger;

    public BasicWebContentProcessor(
        HttpClient httpClient,
        ILogger<BasicWebContentProcessor> logger)
    {
        _httpClient = httpClient ?? throw new ArgumentNullException(nameof(httpClient));
        _logger = logger ?? throw new ArgumentNullException(nameof(logger));
    }

    public async Task<IEnumerable<WebContentChunk>> ProcessAsync(
        string url,
        CrawlOptions? crawlOptions = null,
        ChunkingOptions? chunkingOptions = null,
        CancellationToken cancellationToken = default)
    {
        ArgumentNullException.ThrowIfNull(url);

        var results = new List<WebContentChunk>();
        await foreach (var result in ProcessWithProgressAsync(url, crawlOptions, chunkingOptions, cancellationToken))
        {
            if (result.IsSuccess && result.Result != null)
            {
                results.AddRange(result.Result);
            }
        }

        return results;
    }

    public async IAsyncEnumerable<WebProcessingResult> ProcessWithProgressAsync(
        string url,
        CrawlOptions? crawlOptions = null,
        ChunkingOptions? chunkingOptions = null,
        CancellationToken cancellationToken = default)
    {
        ArgumentNullException.ThrowIfNull(url);

        crawlOptions ??= new CrawlOptions();
        chunkingOptions ??= new ChunkingOptions();

        // Crawl phase
        yield return new WebProcessingResult
        {
            IsSuccess = true,
            Progress = new WebProcessingProgress
            {
                Status = "Crawling",
                CurrentUrl = url,
                PagesProcessed = 0,
                TotalPages = 1
            }
        };

        try
        {

            var crawlResults = await CrawlAsync(url, crawlOptions, cancellationToken);

            // Extract phase
            var extractedContents = new List<RawWebContent>();
            int processed = 0;
            foreach (var crawlResult in crawlResults)
            {
                var rawContent = await ExtractAsync(crawlResult.Url, cancellationToken);
                extractedContents.Add(rawContent);
                processed++;
            }

            // Report extraction progress
            yield return new WebProcessingResult
            {
                IsSuccess = true,
                Progress = new WebProcessingProgress
                {
                    Status = "Extracting",
                    CurrentUrl = url,
                    PagesProcessed = extractedContents.Count,
                    TotalPages = crawlResults.Count()
                }
            };

            // Parse phase
            var parsedContents = new List<ParsedWebContent>();
            foreach (var rawContent in extractedContents)
            {
                var parsedContent = await ParseAsync(rawContent, cancellationToken);
                parsedContents.Add(parsedContent);
            }

            // Report parsing progress
            yield return new WebProcessingResult
            {
                IsSuccess = true,
                Progress = new WebProcessingProgress
                {
                    Status = "Parsing",
                    CurrentUrl = url,
                    PagesProcessed = parsedContents.Count,
                    TotalPages = extractedContents.Count
                }
            };

            // Chunk phase
            var allChunks = new List<WebContentChunk>();
            foreach (var parsedContent in parsedContents)
            {
                var chunks = await ChunkAsync(parsedContent, chunkingOptions, cancellationToken);
                allChunks.AddRange(chunks);
            }

            // Report chunking progress
            yield return new WebProcessingResult
            {
                IsSuccess = true,
                Progress = new WebProcessingProgress
                {
                    Status = "Chunking",
                    CurrentUrl = url,
                    PagesProcessed = parsedContents.Count,
                    TotalPages = parsedContents.Count
                }
            };

            // Final result
            yield return new WebProcessingResult
            {
                IsSuccess = true,
                Result = allChunks,
                Progress = new WebProcessingProgress
                {
                    Status = "Completed",
                    PagesProcessed = parsedContents.Count,
                    TotalPages = parsedContents.Count
                }
            };
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Failed to process URL: {Url}", url);

            yield return new WebProcessingResult
            {
                IsSuccess = false,
                ErrorMessage = ex.Message,
                Exception = ex
            };
        }
    }

    public async Task<IEnumerable<RawWebContent>> CrawlAsync(
        string url,
        CrawlOptions? crawlOptions = null,
        CancellationToken cancellationToken = default)
    {
        ArgumentNullException.ThrowIfNull(url);

        crawlOptions ??= new CrawlOptions();

        // Simple single-page crawl for now
        // In a full implementation, this would handle depth, robots.txt, sitemaps, etc.

        try
        {
            var response = await _httpClient.GetAsync(url, cancellationToken);
            response.EnsureSuccessStatusCode();

            var content = await response.Content.ReadAsStringAsync(cancellationToken);
            var contentType = response.Content.Headers.ContentType?.MediaType ?? "text/html";

            var rawContent = new RawWebContent
            {
                Url = url,
                Content = content,
                ContentType = contentType,
                Metadata = new WebContentMetadata
                {
                    ContentLength = content.Length,
                    LastModified = response.Content.Headers.LastModified?.DateTime,
                    ContentType = contentType
                }
            };

            // Extract headers
            foreach (var header in response.Headers)
            {
                rawContent.Headers[header.Key] = string.Join(", ", header.Value);
            }

            return new[] { rawContent };
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Failed to crawl URL: {Url}", url);
            return Array.Empty<RawWebContent>();
        }
    }

    public async Task<RawWebContent> ExtractAsync(string url, CancellationToken cancellationToken = default)
    {
        ArgumentNullException.ThrowIfNull(url);

        try
        {
            var response = await _httpClient.GetAsync(url, cancellationToken);
            response.EnsureSuccessStatusCode();

            var content = await response.Content.ReadAsStringAsync(cancellationToken);
            var contentType = response.Content.Headers.ContentType?.MediaType ?? "text/html";

            return new RawWebContent
            {
                Url = url,
                Content = content,
                ContentType = contentType,
                Metadata = new WebContentMetadata
                {
                    ContentLength = content.Length,
                    LastModified = response.Content.Headers.LastModified?.DateTime,
                    ContentType = contentType
                }
            };
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Failed to extract content from URL: {Url}", url);
            throw;
        }
    }

    public Task<ParsedWebContent> ParseAsync(RawWebContent rawContent, CancellationToken cancellationToken = default)
    {
        ArgumentNullException.ThrowIfNull(rawContent);

        try
        {
            var parsedContent = new ParsedWebContent
            {
                Url = rawContent.Url,
                Metadata = rawContent.Metadata
            };

            if (rawContent.ContentType.Contains("html"))
            {
                // Parse HTML content
                var doc = new HtmlDocument();
                doc.LoadHtml(rawContent.Content);

                // Extract title
                var titleNode = doc.DocumentNode.SelectSingleNode("//title");
                if (titleNode != null)
                {
                    parsedContent.Metadata.Title = titleNode.InnerText.Trim();
                }

                // Extract meta description
                var descNode = doc.DocumentNode.SelectSingleNode("//meta[@name='description']");
                if (descNode != null)
                {
                    parsedContent.Metadata.Description = descNode.GetAttributeValue("content", "");
                }

                // Extract meta keywords
                var keywordsNode = doc.DocumentNode.SelectSingleNode("//meta[@name='keywords']");
                if (keywordsNode != null)
                {
                    var keywords = keywordsNode.GetAttributeValue("content", "")
                        .Split(',', StringSplitOptions.RemoveEmptyEntries)
                        .Select(k => k.Trim())
                        .ToList();
                    parsedContent.Metadata.Keywords = keywords;
                }

                // Extract text content
                var textNodes = doc.DocumentNode.SelectNodes("//text()[normalize-space(.) != '']");
                if (textNodes != null)
                {
                    var textContent = string.Join(" ", textNodes
                        .Where(n => !IsScriptOrStyle(n))
                        .Select(n => n.InnerText.Trim())
                        .Where(t => !string.IsNullOrWhiteSpace(t)));

                    parsedContent.Content = CleanText(textContent);
                }

                // Extract links
                var linkNodes = doc.DocumentNode.SelectNodes("//a[@href]");
                if (linkNodes != null)
                {
                    parsedContent.ExtractedLinks = linkNodes
                        .Select(n => n.GetAttributeValue("href", ""))
                        .Where(href => !string.IsNullOrWhiteSpace(href))
                        .ToList();
                }

                // Extract images
                var imgNodes = doc.DocumentNode.SelectNodes("//img[@src]");
                if (imgNodes != null)
                {
                    parsedContent.ExtractedImages = imgNodes
                        .Select(n => n.GetAttributeValue("src", ""))
                        .Where(src => !string.IsNullOrWhiteSpace(src))
                        .ToList();
                }
            }
            else
            {
                // For non-HTML content, use raw content
                parsedContent.Content = rawContent.Content;
            }

            return Task.FromResult(parsedContent);
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Failed to parse content from URL: {Url}", rawContent.Url);
            throw;
        }
    }

    public Task<IEnumerable<WebContentChunk>> ChunkAsync(
        ParsedWebContent parsedContent,
        ChunkingOptions? chunkingOptions = null,
        CancellationToken cancellationToken = default)
    {
        ArgumentNullException.ThrowIfNull(parsedContent);

        chunkingOptions ??= new ChunkingOptions();

        try
        {
            var chunks = new List<WebContentChunk>();
            var content = parsedContent.Content;

            if (string.IsNullOrWhiteSpace(content))
            {
                return Task.FromResult<IEnumerable<WebContentChunk>>(chunks);
            }

            // Simple fixed-size chunking
            var chunkSize = chunkingOptions.MaxChunkSize;
            var overlap = chunkingOptions.OverlapSize;

            for (int i = 0; i < content.Length; i += chunkSize - overlap)
            {
                var end = Math.Min(i + chunkSize, content.Length);
                var chunkContent = content.Substring(i, end - i);

                // Try to break at word boundaries
                if (end < content.Length && !char.IsWhiteSpace(content[end]))
                {
                    var lastSpace = chunkContent.LastIndexOf(' ');
                    if (lastSpace > chunkContent.Length * 0.8) // Only if we don't lose too much
                    {
                        chunkContent = chunkContent.Substring(0, lastSpace);
                    }
                }

                var chunk = new WebContentChunk
                {
                    Content = chunkContent.Trim(),
                    SourceUrl = parsedContent.Url,
                    ChunkIndex = chunks.Count,
                    Quality = CalculateQuality(chunkContent),
                    Strategy = chunkingOptions.Strategy,
                    Metadata = parsedContent.Metadata
                };

                chunks.Add(chunk);

                if (end >= content.Length) break;
            }

            return Task.FromResult<IEnumerable<WebContentChunk>>(chunks);
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Failed to chunk content from URL: {Url}", parsedContent.Url);
            throw;
        }
    }

    private static bool IsScriptOrStyle(HtmlNode node)
    {
        var parent = node.ParentNode;
        while (parent != null)
        {
            if (parent.Name.ToLower() is "script" or "style" or "noscript")
                return true;
            parent = parent.ParentNode;
        }
        return false;
    }

    private static string CleanText(string text)
    {
        // Remove extra whitespace
        text = Regex.Replace(text, @"\s+", " ");
        // Remove common HTML entities that might have been missed
        text = System.Net.WebUtility.HtmlDecode(text);
        return text.Trim();
    }

    private static double CalculateQuality(string content)
    {
        if (string.IsNullOrWhiteSpace(content))
            return 0.0;

        // Simple quality scoring based on:
        // - Length (not too short, not too long)
        // - Word count
        // - Sentence structure

        var length = content.Length;
        var wordCount = content.Split(' ', StringSplitOptions.RemoveEmptyEntries).Length;
        var sentenceCount = content.Split('.', '!', '?', StringSplitOptions.RemoveEmptyEntries).Length;

        double score = 0.5; // Base score

        // Length score
        if (length >= 100 && length <= 2000)
            score += 0.2;
        else if (length < 50)
            score -= 0.2;

        // Word count score
        if (wordCount >= 20 && wordCount <= 300)
            score += 0.2;

        // Sentence structure score
        if (sentenceCount > 1 && wordCount / sentenceCount >= 5)
            score += 0.1;

        return Math.Max(0.0, Math.Min(1.0, score));
    }
}